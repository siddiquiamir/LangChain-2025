{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ece0ea-015c-435b-b584-108889a70514",
   "metadata": {},
   "source": [
    "- You may find yourself in a situation where you are getting rate limited by the model provider API because you're making too many requests.\n",
    "- For example, this might happen if you are running many parallel queries to benchmark the chat model on a test dataset.\n",
    "- If you are facing such a situation, you can use a rate limiter to help match the rate at which you're making request to the rate allowed by the API.\n",
    "\n",
    "- Langchain comes with a built-in in memory rate limiter.\n",
    "- The provided rate limiter can only limit the number of requests per unit time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8036f826-ae13-4e89-a014-17a638ed243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.1,  # <-- Super slow! We can only make a request once every 10 seconds!!\n",
    "    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,\n",
    "    max_bucket_size=10,  # Controls the maximum burst size.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee6654d5-4b6c-4979-a996-7d5128b95690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962ef227-cf33-4922-ba78-02f44204fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for Groq:  ········\n"
     ]
    }
   ],
   "source": [
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a12f4b-f493-4d55-aaf4-e0990f3c6993",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6134f23-27b2-4d65-b16c-94ab7bd4fc35",
   "metadata": {},
   "source": [
    "- Let's confirm that the rate limiter works. We should only be able to invoke the model once per 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "645009af-e930-4ffe-8309-f2690cfb229b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6157643795013428\n",
      "0.33002495765686035\n",
      "0.3189585208892822\n",
      "0.3052046298980713\n",
      "0.7480382919311523\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for _ in range(5):\n",
    "    tic = time.time()\n",
    "    model.invoke(\"hello\")\n",
    "    toc = time.time()\n",
    "    print(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25208c04-2aa0-43c9-966e-f54a7fac5f69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
